{
  "chunk-ea7adb1b84305db087229b6680ff13ac": {
    "tokens": 548,
    "content": "The primary challenge in the development of large-scale artificial intelligence (AI) systems lies in achieving scalable decision-making— extending the AI models while maintaining sufficient performance. Existing research indicates that distributed AI can improve scalability by decomposing complex tasks and distributing them across collaborative nodes. However, previous technologies suffered from compromised real-world applicability and scalability due to the massive requirement of communication and sampled data. Here we develop a model-based decentralized policy optimization framework, which can be efficiently deployed in multi-agent systems. By leveraging local observation through the agent-level topological decoupling of global dynamics, we prove that this decentralized mechanism achieves accurate estimations of global information. Importantly, we further introduce model learning to reinforce the optimal policy for monotonic improvement with a limited amount of sampled data. Empirical results on diverse scenarios show the superior scalability of our approach, particularly in real-world systems with hundreds of agents, thereby paving the way for scaling up AI systems.\nAchieving scalable decision-making becomes a critical challenge when deploying artificial intelligence (AI) models into large-scale systems1. First, this requires effective information exchange among system enti- ties to help an agent perceive the state of the environment and other agents2. However, owing to constraints and high costs associated with communication3, achieving comprehensive information sharing across the entire system becomes unfeasible (Fig. 1a). For example, in traffic networks, frequent and extensive communication between traffic lights can cause a substantial power loss (Fig. 1b). In some sys- tems involving user information, centralized information collection also increases the risk of privacy leakage4. Second, efficient sample acquisition and learning from limited data are necessary because the cost of agent–environment interaction rises exponentially with the scale of the system5, and in some scenarios, only limited interactions are permitted. Therefore, developing an effective communicationmode and a sample-efficient approach is crucial for achieving scalable decision-making.\nAs an advanced paradigm of distributed AI, multi-agent reinforce- ment learning (MARL) provides a possible solution6,7 and has made progress in various scenarios, including autonomous driving8–12, wire- less communications13,14, multi-player games15,16, power systems17,18 and urban transportation19–24. The advantage of MARL lies in its ability to perform nonlinear fitting solely by data and attain efficient inference. In contrast, traditional methods such as model predictive control (MPC) require precise system dynamics25,26, which are often difficult to obtain in complex systems. Even with accurate dynamics, traditional methods often rely on system linearization27, which inevitably harms performance by disregarding numerous intricate nonlinear factors and system per- turbations inherent in complex systems. In addition, deficiencies in computational efficiency, numerical stability and communication costs",
    "chunk_order_index": 0,
    "full_doc_id": "doc-ea7adb1b84305db087229b6680ff13ac"
  }
}